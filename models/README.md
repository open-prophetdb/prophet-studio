## Awesome Models for NER and RE

### NER

- [BERN2: an advanced neural biomedical namedentity recognition and normalization tool](https://github.com/dmis-lab/BERN2)

BERN2 is a tool that improves the previous neural network-based NER tool by employing a multi-task NER model and neural network-based NEN models to achieve much faster and more accurate inference. This repository provides a way to host your own BERN2 server. Currently, BERN2 is running on a hosting server with 64-core CPU, 512GB Memory, and 12GB GPU. See the [paper](https://arxiv.org/abs/2201.02080) for more details.

- [BioBERT: PyTorch Implementation of BioBERT](https://github.com/dmis-lab/biobert-pytorch)

This repository provides the PyTorch implementation of BioBERT. You can easily use BioBERT with transformers. This project is supported by the members of DMIS-Lab @ Korea University including Jinhyuk Lee, Wonjin Yoon, Minbyul Jeong, Mujeen Sung, and Gangwoo Kim.

- [BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://github.com/dmis-lab/biobert)

This repository provides the code for fine-tuning BioBERT, a biomedical language representation model designed for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, question answering, etc. Please refer to our paper BioBERT: a pre-trained biomedical language representation model for biomedical text mining for more details. This project is done by DMIS-Lab.

- [bio-lm: Biomedical and Clinical Language Models](https://github.com/facebookresearch/bio-lm)

A large array of pretrained models are available to the biomedical NLP (BioNLP) community. Finding the best model for a particular task can be difficult and time-consuming. For many applications in the biomedical and clinical domains, it is crucial that models can be built quickly and are highly accurate. We present a large-scale study across 18 established biomedical and clinical NLP tasks to determine which of several popular open-source biomedical and clinical NLP models work well in different settings. Furthermore, we apply recent advances in pretraining to train new biomedical language models, and carefully investigate the effect of various design choices on downstream performance. Our best models perform well in all of our benchmarks, and set new State-of-the-Art in 9 tasks. We release these models in the hope that they can help the community to speed up and increase the accuracy of BioNLP and text mining applications. [https://aclanthology.org/2020.clinicalnlp-1.17/](https://aclanthology.org/2020.clinicalnlp-1.17/)


### RE


## Blogs

1. [BERT Based Named Entity Recognition (NER) Tutorial And Demo](https://www.pragnakalp.com/bert-named-entity-recognition-ner-tutorial-demo/)

2. [Named Entity Recognition (NER) Using BIOBERT](https://www.pragnakalp.com/named-entity-recognition-ner-using-biobert-demo/)